"""
GSW MLP End-to-End (Streamlit)

Objetivo: Treinar MLP para prever vit√≥ria/derrota do Golden State Warriors (GSW) e
comparar com Regress√£o Linear e Log√≠stica. Gera gr√°ficos, m√©tricas e relat√≥rio.

Principais escolhas e justificativas:
- Features: reutilizamos as mesmas da Atividade 1 via FeatureEngineer (m√©dias hist√≥ricas,
  √∫ltimos 5 jogos, desvio, jogo anterior etc.), pois capturam forma recente e estabilidade.
- Arquitetura MLP (padr√£o): 2 camadas ocultas [64, 32], ativa√ß√£o ReLU. ReLU √© est√°vel e
  eficiente em conjuntos tabulares. Sa√≠da com sigmoid para bin√°rio (vit√≥ria=1/derrota=0).
- Regulariza√ß√£o: EarlyStopping (paciente=20) e Validation Split (20%). Dropout e BatchNorm
  opcionais (√∫teis contra overfitting; desativados por padr√£o para simplicidade).
- √âpocas: at√© 300, mas EarlyStopping interrompe antes quando n√£o h√° melhora em val_loss.
- Otimizadores: SGD, Adam e RMSprop para compara√ß√£o conforme solicitado.
- M√©tricas: MAE, RMSE e R¬≤ sobre as probabilidades previstas vs r√≥tulos bin√°rios (0/1) e
  tamb√©m mostramos Accuracy, Matriz de Confus√£o e ROC opcional.
- Robustez: tenta carregar dados da NBA; se indispon√≠vel/insuficiente, gera dados sint√©ticos
  coerentes com o formato exigido (sem travar por falta de dados).

Como executar:
- No PowerShell (Windows):
    streamlit run Atividade_nba/gsw_mlp_app.py
"""

import os
import sys
import math
import time
import json
import random
import warnings
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings("ignore")

# Acesso aos m√≥dulos locais da Atividade 1
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, CURRENT_DIR)
from feature_engineering import FeatureEngineer
from regression_models import LinearRegressionModel, LogisticRegressionModel
# Importa√ß√µes que dependem do pacote nba_api podem falhar em ambientes sem setup.
# Para robustez, importaremos NBADataCollector sob demanda nas fun√ß√µes com try/except.

# Keras / TensorFlow
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
except Exception as e:
    st.error("TensorFlow/Keras n√£o est√° instalado. Instale 'tensorflow' para rodar a MLP.")
    raise

# ----------------------------- Utilidades Gerais -----------------------------

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def metrics_regression_like(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """Calcula MAE, RMSE, R¬≤ tratando y como valores cont√≠nuos (0/1 ou reais)."""
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    mae = float(np.mean(np.abs(y_true - y_pred)))
    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
    # R¬≤: 1 - SS_res/SS_tot; se var(y)=0, definir R¬≤=0
    var_y = float(np.var(y_true))
    if var_y < 1e-12:
        r2 = 0.0
    else:
        ss_res = float(np.sum((y_true - y_pred) ** 2))
        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))
        r2 = 1.0 - (ss_res / (ss_tot + 1e-12))
    return {"mae": mae, "rmse": rmse, "r2": r2}


def binarize_from_proba(p: np.ndarray, threshold: float = 0.5) -> np.ndarray:
    return (p.reshape(-1) >= threshold).astype(int)


def ci95(values: np.ndarray) -> Tuple[float, float]:
    """Intervalo de confian√ßa 95% para a m√©dia dos valores (normal approx)."""
    v = np.asarray(values).reshape(-1)
    m = float(np.mean(v))
    se = float(np.std(v, ddof=1) / max(1, math.sqrt(len(v))))
    return (m - 1.96 * se, m + 1.96 * se)

# ----------------------------- Dados / Fallback ------------------------------

TEAM_ID_GSW = 1610612744
GSW_PLAYERS = [
    "Stephen Curry", "Klay Thompson", "Draymond Green", "Jonathan Kuminga",
    "Andrew Wiggins", "Chris Paul"
]


def try_load_team_log(season: str) -> Optional[pd.DataFrame]:
    try:
        from data_collector import NBADataCollector  # lazy import
        collector = NBADataCollector(season=season)
        df = collector.get_team_game_log(TEAM_ID_GSW)
        return df
    except Exception:
        return None


def try_load_player_logs(season: str, players: List[str]) -> Dict[str, pd.DataFrame]:
    out = {}
    try:
        from data_collector import NBADataCollector  # lazy import
        collector = NBADataCollector(season=season)
        for name in players:
            player = collector.find_gsw_player(name)
            if player is None:
                out[name] = None
                continue
            df = collector.get_player_game_log(player["id"]) if isinstance(player, dict) else None
            out[name] = df
            time.sleep(0.4)
    except Exception:
        for name in players:
            out[name] = None
    return out


def synthetic_team_log(n_games: int = 60, season: str = "2023-24") -> pd.DataFrame:
    rng = np.random.default_rng(42)
    dates = pd.date_range("2023-10-01", periods=n_games, freq="3D")
    pts = rng.normal(114, 10, n_games).clip(90, 150)
    opp_pts = rng.normal(111, 10, n_games).clip(85, 150)
    wl = np.where(pts >= opp_pts, "W", "L")
    plus_minus = pts - opp_pts
    fgm = (pts * 0.4 + rng.normal(0, 2, n_games)).clip(30, 60)
    fga = fgm + rng.normal(8, 3, n_games).clip(5, 25)
    fg_pct = (fgm / np.maximum(fga, 1)).clip(0.35, 0.6)
    fg3m = (pts * 0.13 + rng.normal(0, 1.5, n_games)).clip(5, 25)
    fg3a = fg3m + rng.normal(5, 2, n_games).clip(2, 15)
    fg3_pct = (fg3m / np.maximum(fg3a, 1)).clip(0.25, 0.55)
    ftm = (pts * 0.18 + rng.normal(0, 2, n_games)).clip(5, 30)
    fta = ftm + rng.normal(3, 2, n_games).clip(1, 15)
    ft_pct = (ftm / np.maximum(fta, 1)).clip(0.6, 0.95)
    reb = rng.normal(45, 6, n_games).clip(30, 60)
    ast = rng.normal(26, 5, n_games).clip(15, 40)
    oreb = rng.normal(10, 3, n_games).clip(5, 20)
    dreb = reb - oreb
    stl = rng.normal(7, 2, n_games).clip(3, 15)
    blk = rng.normal(5, 1.5, n_games).clip(1, 10)
    tov = rng.normal(14, 3, n_games).clip(8, 22)
    pf = rng.normal(20, 3, n_games).clip(12, 30)
    min_ = np.full(n_games, 240)  # total team minutes

    matchups = [f"GSW {'vs.' if i % 2 == 0 else '@'} OPP" for i in range(n_games)]
    df = pd.DataFrame({
        "GAME_DATE": dates.strftime("%Y-%m-%d"),
        "MATCHUP": matchups,
        "WL": wl,
        "PTS": pts,
        "REB": reb,
        "AST": ast,
        "FGM": fgm,
        "FGA": fga,
        "FG_PCT": fg_pct,
        "FG3M": fg3m,
        "FG3A": fg3a,
        "FG3_PCT": fg3_pct,
        "FTM": ftm,
        "FTA": fta,
        "FT_PCT": ft_pct,
        "OREB": oreb,
        "DREB": dreb,
        "STL": stl,
        "BLK": blk,
        "TOV": tov,
        "PF": pf,
        "MIN": min_,
    })
    return df


def synthetic_player_log(n_games: int = 40, name: str = "Stephen Curry") -> pd.DataFrame:
    rng = np.random.default_rng(abs(hash(name)) % (2**32))
    dates = pd.date_range("2023-10-05", periods=n_games, freq="4D")
    base_pts = 30 if "Curry" in name else (22 if "Klay" in name else 12)
    base_ast = 6 if "Draymond" in name else (4 if "Paul" in name else 2)
    base_reb = 5 if "Kuminga" in name else 4
    pts = rng.normal(base_pts, 6, n_games).clip(0, 60)
    ast = rng.normal(base_ast, 2, n_games).clip(0, 15)
    reb = rng.normal(base_reb, 3, n_games).clip(0, 20)
    fgm = (pts * 0.4 + rng.normal(0, 1.5, n_games)).clip(1, 25)
    fga = fgm + rng.normal(6, 2, n_games).clip(2, 20)
    fg_pct = (fgm / np.maximum(fga, 1)).clip(0.3, 0.7)
    fg3m = (pts * 0.35/3 + rng.normal(0, 1.0, n_games)).clip(0, 10)
    fg3a = fg3m + rng.normal(3, 1.5, n_games).clip(0, 15)
    fg3_pct = (fg3m / np.maximum(fg3a, 1)).clip(0.25, 0.6)
    ftm = (pts * 0.2 + rng.normal(0, 1, n_games)).clip(0, 15)
    fta = ftm + rng.normal(2, 1, n_games).clip(0, 10)
    ft_pct = (ftm / np.maximum(fta, 1)).clip(0.5, 1.0)
    oreb = rng.normal(1.2, 0.8, n_games).clip(0, 6)
    dreb = reb - oreb
    stl = rng.normal(1.3, 0.6, n_games).clip(0, 5)
    blk = rng.normal(0.5, 0.5, n_games).clip(0, 4)
    tov = rng.normal(2.8, 1.2, n_games).clip(0, 8)
    pf = rng.normal(2.2, 0.9, n_games).clip(0, 6)
    plus_minus = rng.normal(3, 10, n_games)
    min_ = rng.normal(32, 5, n_games).clip(10, 48)

    wl = np.where(plus_minus >= 0, "W", "L")
    matchups = [f"GSW {'vs.' if i % 2 == 0 else '@'} OPP" for i in range(n_games)]

    df = pd.DataFrame({
        "GAME_DATE": dates.strftime("%Y-%m-%d"),
        "MATCHUP": matchups,
        "WL": wl,
        "PTS": pts,
        "REB": reb,
        "AST": ast,
        "FGM": fgm,
        "FGA": fga,
        "FG_PCT": fg_pct,
        "FG3M": fg3m,
        "FG3A": fg3a,
        "FG3_PCT": fg3_pct,
        "FTM": ftm,
        "FTA": fta,
        "FT_PCT": ft_pct,
        "OREB": oreb,
        "DREB": dreb,
        "STL": stl,
        "BLK": blk,
        "TOV": tov,
        "PF": pf,
        "MIN": min_,
        "PLUS_MINUS": plus_minus,
    })
    return df

# ----------------------------- MLP Construction ------------------------------

@dataclass
class MLPConfig:
    hidden_layers: List[int] = None
    activation: str = "relu"
    dropout: float = 0.0
    batch_norm: bool = False
    epochs: int = 300
    batch_size: int = 32
    validation_split: float = 0.2
    patience: int = 20

    def __post_init__(self):
        if self.hidden_layers is None:
            self.hidden_layers = [64, 32]


def build_mlp(input_dim: int, cfg: MLPConfig) -> keras.Model:
    model = keras.Sequential()
    model.add(layers.Input(shape=(input_dim,)))
    for units in cfg.hidden_layers:
        model.add(layers.Dense(units, activation=cfg.activation))
        if cfg.batch_norm:
            model.add(layers.BatchNormalization())
        if cfg.dropout and cfg.dropout > 0:
            model.add(layers.Dropout(cfg.dropout))
    model.add(layers.Dense(1, activation="sigmoid"))
    return model


def train_mlp(X: np.ndarray, y: np.ndarray, optimizer: str, cfg: MLPConfig, random_state: int = 42):
    set_seed(random_state)
    model = build_mlp(X.shape[1], cfg)
    if optimizer.lower() == "sgd":
        opt = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)
    elif optimizer.lower() == "adam":
        opt = keras.optimizers.Adam(learning_rate=1e-3)
    elif optimizer.lower() == "rmsprop":
        opt = keras.optimizers.RMSprop(learning_rate=1e-3)
    else:
        raise ValueError("Optimizer deve ser um de: SGD, Adam, RMSprop")

    model.compile(optimizer=opt, loss="binary_crossentropy", metrics=["accuracy"])

    es = keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=cfg.patience, restore_best_weights=True
    )

    history = model.fit(
        X, y,
        validation_split=cfg.validation_split,
        epochs=cfg.epochs,
        batch_size=cfg.batch_size,
        callbacks=[es],
        verbose=0,
    )
    return model, history

# ------------------------------- Visualiza√ß√µes -------------------------------

def plot_training_history(histories: Dict[str, keras.callbacks.History]):
    fig, ax = plt.subplots(1, 2, figsize=(12, 4))
    for name, h in histories.items():
        ax[0].plot(h.history.get("loss", []), label=f"{name} - loss")
        ax[0].plot(h.history.get("val_loss", []), label=f"{name} - val_loss", linestyle=":")
        ax[1].plot(h.history.get("accuracy", []), label=f"{name} - acc")
        ax[1].plot(h.history.get("val_accuracy", []), label=f"{name} - val_acc", linestyle=":")
    ax[0].set_title("Evolu√ß√£o do Erro (Loss)")
    ax[0].set_xlabel("√âpoca")
    ax[0].set_ylabel("Loss")
    ax[0].legend()
    ax[1].set_title("Evolu√ß√£o da Acur√°cia")
    ax[1].set_xlabel("√âpoca")
    ax[1].set_ylabel("Accuracy")
    ax[1].legend()
    fig.tight_layout()
    return fig


def plot_error_matrix(y_true: np.ndarray, y_pred: np.ndarray, name: str = "Modelo"):
    errors = y_true.reshape(-1) - y_pred.reshape(-1)
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    sns.histplot(errors, kde=True, ax=axes[0], color="#1976d2")
    axes[0].set_title(f"Histograma dos Erros ({name})")
    axes[0].set_xlabel("Erro (y - y_pred)")

    axes[1].scatter(y_true, y_pred, alpha=0.6)
    axes[1].plot([0, 1], [0, 1], "r--")
    axes[1].set_title("Scatter Previsto vs Real")
    axes[1].set_xlabel("Real (0/1)")
    axes[1].set_ylabel("Previsto (prob)")

    axes[2].scatter(range(len(errors)), errors, alpha=0.6)
    axes[2].axhline(0, color="r", linestyle="--")
    axes[2].set_title("Erro vs Amostra")
    axes[2].set_xlabel("√çndice da amostra")
    axes[2].set_ylabel("Erro")

    fig.tight_layout()
    return fig


def plot_players_prediction_table(df_table: pd.DataFrame):
    fig, ax = plt.subplots(figsize=(10, 0.6 * len(df_table) + 2))
    ax.axis("off")
    # df_table cont√©m colunas de texto (e.g., "PTS (Prev|Real)") ‚Äì n√£o arredondar
    cell_text = df_table.astype(str).values
    tbl = ax.table(cellText=cell_text, colLabels=list(df_table.columns), loc="center")
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(10)
    tbl.scale(1, 1.2)
    return fig

# ------------------------------- App Principal -------------------------------

st.set_page_config(page_title="GSW MLP", page_icon="üèÄ", layout="wide")
st.title("üèÄ GSW - MLP vs Regress√µes (Vit√≥ria/Derrota)")
st.caption("Treina 3 MLPs (SGD/Adam/RMSprop), gera gr√°ficos e compara com baselines.")

with st.sidebar:
    st.header("Configura√ß√µes")
    season = st.selectbox("Temporada", ["2024-25", "2023-24", "2022-23"], index=1)
    st.markdown("---")
    st.subheader("Arquitetura MLP")
    hidden_1 = st.number_input("Neuronios camada 1", min_value=1, max_value=256, value=10, step=1)
    hidden_2 = st.number_input("Neuronios camada 2", min_value=0, max_value=256, value=5, step=1)
    act = st.selectbox("Ativa√ß√£o", ["relu", "tanh", "sigmoid"], index=0)
    use_dropout = st.checkbox("Usar Dropout", value=True)
    dropout_p = st.slider("Taxa Dropout", 0.0, 0.8, 0.20, 0.05, disabled=not use_dropout)
    use_bn = st.checkbox("Usar BatchNorm", value=False)
    st.markdown("---")
    epochs = st.slider("√âpocas m√°x.", 50, 500, 430, 10)
    patience = st.slider("EarlyStopping paci√™ncia", 5, 50, 20, 1)
    batch_size = st.selectbox("Batch size", [16, 32, 64, 128], index=1)
    val_split = st.slider("Validation split", 0.1, 0.4, 0.20, 0.05)
    st.markdown("---")
    train_button = st.button("Treinar MLPs")

# 1) Carregar dados do time (ou sint√©ticos)
with st.spinner("Carregando dados do GSW (time) ou gerando sint√©ticos..."):
    team_log = try_load_team_log(season)
    if team_log is None or len(team_log) < 10:
        team_log = synthetic_team_log(n_games=60, season=season)
        st.info("Usando dados sint√©ticos de time (fallback).")

# 2) Feature engineering do time
fe = FeatureEngineer()
team_features = fe.create_team_features(team_log)
if team_features is None or team_features.empty:
    st.error("N√£o foi poss√≠vel criar features do time. Encerrando.")
    st.stop()

# Sele√ß√£o de features (da Atividade 1)
available_features = fe.get_available_features(entity_type='team')
selected_features = [
    "avg_pts", "avg_reb", "avg_ast", "avg_fg_pct", "avg_fg3m",
    "pts_last_5", "reb_last_5", "ast_last_5", "std_pts", "prev_pts",
]
selected_features = [f for f in selected_features if f in available_features]

X_all = team_features[selected_features].fillna(team_features[selected_features].median())
y_all = team_features["target_victory"].astype(int)

# Padronizar
from sklearn.preprocessing import StandardScaler
scaler_team = StandardScaler()
X_scaled = scaler_team.fit_transform(X_all.values)

# Split train/test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_all.values, test_size=0.2, random_state=42, stratify=y_all.values
)

st.subheader("Amostras e Features (Time)")
col_a, col_b = st.columns([1, 2])
with col_a:
    st.metric("Amostras", f"{len(X_all)}")
    st.metric("Features", f"{X_all.shape[1]}")
with col_b:
    st.write(pd.DataFrame({"Feature": selected_features}))

# 3) Treinar MLPs
cfg = MLPConfig(
    hidden_layers=[hidden_1] + ([hidden_2] if hidden_2 > 0 else []),
    activation=act,
    dropout=(dropout_p if use_dropout else 0.0),
    batch_norm=use_bn,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=val_split,
    patience=patience,
)

mlp_models = {}
mlp_histories = {}
mlp_preds = {}

if train_button:
    with st.spinner("Treinando MLPs (SGD, Adam, RMSprop)..."):
        for opt in ["SGD", "Adam", "RMSprop"]:
            model, history = train_mlp(X_train, y_train, optimizer=opt, cfg=cfg)
            proba = model.predict(X_test, verbose=0).reshape(-1)
            mlp_models[opt] = model
            mlp_histories[opt] = history
            mlp_preds[opt] = proba
    st.success("Treino conclu√≠do.")

# Se n√£o treinou agora, treine r√°pido padr√£o (para permitir gr√°ficos)
if not mlp_models:
    model, history = train_mlp(X_train, y_train, optimizer="Adam", cfg=cfg)
    proba = model.predict(X_test, verbose=0).reshape(-1)
    mlp_models["Adam"] = model
    mlp_histories["Adam"] = history
    mlp_preds["Adam"] = proba

# 4) M√©tricas MLP
st.subheader("M√©tricas MLP (sobre probabilidades)")
metrics_rows = []
for name, proba in mlp_preds.items():
    regm = metrics_regression_like(y_test, proba)
    acc = float(np.mean(binarize_from_proba(proba) == y_test))
    metrics_rows.append({
        "Modelo": f"MLP-{name}",
        "MAE": regm["mae"],
        "RMSE": regm["rmse"],
        "R¬≤": regm["r2"],
        "Accuracy": acc,
    })
metrics_df = pd.DataFrame(metrics_rows)
st.dataframe(metrics_df.style.format({"MAE": "{:.3f}", "RMSE": "{:.3f}", "R¬≤": "{:.3f}", "Accuracy": "{:.3f}"}), use_container_width=True)

# 5) Gr√°fico Evolu√ß√£o do Erro (loss/accuracy)
fig_hist = plot_training_history(mlp_histories)
st.pyplot(fig_hist, use_container_width=True)

# 6) Matriz de Erros (hist, scatter, erro x amostra) para melhor MLP
best_name = max(metrics_rows, key=lambda d: d["Accuracy"]) ["Modelo"]
best_key = best_name.split("-")[-1]
fig_err = plot_error_matrix(y_test, mlp_preds[best_key], name=best_name)
st.pyplot(fig_err, use_container_width=True)

# 7) Intervalo de Confian√ßa 95% das probabilidades previstas (teste)
lo, hi = ci95(mlp_preds[best_key])
st.info(f"95% CI da probabilidade prevista m√©dia (teste): [{lo:.3f}, {hi:.3f}]")

# 8) Baselines (Atividade 1): Regress√£o Log√≠stica e Linear (num√©rico 0/1)
st.subheader("Compara√ß√£o com Baselines (Atividade 1)")
X_df_train = pd.DataFrame(X_train, columns=selected_features)
X_df_test  = pd.DataFrame(X_test, columns=selected_features)

# Logistic (classifica√ß√£o bin√°ria)
log_model = LogisticRegressionModel().fit(X_df_train, pd.Series(y_train), scale_features=False)
log_eval = log_model.evaluate(X_df_test, pd.Series(y_test))
log_proba = log_eval["probabilities"][:, 1]
log_acc = float(log_eval["accuracy"])  # j√° calculado
log_reg = metrics_regression_like(y_test, log_proba)

# Linear (regress√£o para 0/1)
lin_model = LinearRegressionModel().fit(X_df_train, pd.Series(y_train), scale_features=False)
lin_pred = lin_model.predict(X_df_test).reshape(-1)
lin_reg = metrics_regression_like(y_test, lin_pred)
lin_acc = float(np.mean((lin_pred >= 0.5).astype(int) == y_test))

compare_rows = [
    {"Modelo": "Regress√£o Log√≠stica", "MAE": log_reg["mae"], "RMSE": log_reg["rmse"], "R¬≤": log_reg["r2"], "Accuracy": log_acc},
    {"Modelo": "Regress√£o Linear",    "MAE": lin_reg["mae"], "RMSE": lin_reg["rmse"], "R¬≤": lin_reg["r2"], "Accuracy": lin_acc},
]
# Melhor MLP
best_acc = -1
best_key_acc = None
for name, proba in mlp_preds.items():
    acc = float(np.mean(binarize_from_proba(proba) == y_test))
    if acc > best_acc:
        best_acc = acc
        best_key_acc = name
best_reg = metrics_regression_like(y_test, mlp_preds[best_key_acc])
compare_rows.append({"Modelo": f"MLP-{best_key_acc}", "MAE": best_reg["mae"], "RMSE": best_reg["rmse"], "R¬≤": best_reg["r2"], "Accuracy": best_acc})

compare_df = pd.DataFrame(compare_rows)
st.dataframe(compare_df.style.format({"MAE": "{:.3f}", "RMSE": "{:.3f}", "R¬≤": "{:.3f}", "Accuracy": "{:.3f}"}), use_container_width=True)

# 9) Gr√°ficos de compara√ß√£o (barras de Accuracy)
fig_comp, ax = plt.subplots(figsize=(6, 4))
ax.bar(compare_df["Modelo"], compare_df["Accuracy"], color=["#546e7a", "#90a4ae", "#1976d2"]) 
ax.set_title("Compara√ß√£o de Accuracy: Log√≠stica vs Linear vs MLP")
ax.set_ylabel("Accuracy (teste)")
ax.set_ylim(0, 1)
for i, v in enumerate(compare_df["Accuracy"].values):
    ax.text(i, v + 0.01, f"{v:.2f}", ha="center")
st.pyplot(fig_comp, use_container_width=True)

# 10) Previs√£o x Realidade para jogadores (pontos, ast, reb) usando regress√£o linear
st.subheader("Previs√£o x Realidade (Jogadores do GSW)")
with st.spinner("Carregando logs de jogadores (ou gerando sint√©ticos)..."):
    plogs = try_load_player_logs(season, GSW_PLAYERS)

player_rows = []
for name in GSW_PLAYERS:
    pdf = plogs.get(name)
    if pdf is None or pdf is None or (hasattr(pdf, 'empty') and pdf.empty):
        pdf = synthetic_player_log(40, name)
    pf = FeatureEngineer().create_player_features(pdf)
    if pf is None or pf.empty:
        continue
    feats = FeatureEngineer().get_available_features(entity_type='player')
    feats = [f for f in feats if f in pf.columns]
    # Treinar 3 modelos lineares (pts, ast, reb)
    Xp = pf[feats].fillna(pf[feats].median())
    yp_pts = pf["target_pts"]
    yp_ast = pf["target_ast"]
    yp_reb = pf["target_reb"]
    # Split simples: √∫ltimos 5 como teste
    if len(Xp) < 10:
        continue
    split_idx = int(len(Xp) * 0.8)
    Xtr, Xte = Xp.iloc[:split_idx], Xp.iloc[split_idx:]
    ytr_pts, yte_pts = yp_pts.iloc[:split_idx], yp_pts.iloc[split_idx:]
    ytr_ast, yte_ast = yp_ast.iloc[:split_idx], yp_ast.iloc[split_idx:]
    ytr_reb, yte_reb = yp_reb.iloc[:split_idx], yp_reb.iloc[split_idx:]

    m_pts = LinearRegressionModel().fit(Xtr, ytr_pts)
    m_ast = LinearRegressionModel().fit(Xtr, ytr_ast)
    m_reb = LinearRegressionModel().fit(Xtr, ytr_reb)

    p_pts = m_pts.predict(Xte)
    p_ast = m_ast.predict(Xte)
    p_reb = m_reb.predict(Xte)

    # Considerar a m√©dia das √∫ltimas previs√µes e reais (representativo)
    row = {
        "Jogador": name,
        "PTS (Prev|Real)": f"{np.mean(p_pts):.1f} | {np.mean(yte_pts):.1f}",
        "AST (Prev|Real)": f"{np.mean(p_ast):.1f} | {np.mean(yte_ast):.1f}",
        "REB (Prev|Real)": f"{np.mean(p_reb):.1f} | {np.mean(yte_reb):.1f}",
        "Pred_PTS": float(np.mean(p_pts)),
    }
    player_rows.append(row)

if player_rows:
    table_df = pd.DataFrame(player_rows)[["Jogador", "PTS (Prev|Real)", "AST (Prev|Real)", "REB (Prev|Real)"]]
    st.dataframe(table_df, use_container_width=True)
    fig_tbl = plot_players_prediction_table(table_df)
    st.pyplot(fig_tbl, use_container_width=True)

    # Ranking por previs√£o de pontos
    rank_df = pd.DataFrame(player_rows).sort_values("Pred_PTS", ascending=False)[["Jogador", "Pred_PTS"]]
    st.subheader("Ranking de Previs√µes (Pontos)")
    st.dataframe(rank_df.reset_index(drop=True), use_container_width=True)
    fig_rank, ax = plt.subplots(figsize=(6, 4))
    ax.barh(rank_df["Jogador"], rank_df["Pred_PTS"], color="#1976d2")
    ax.invert_yaxis()
    ax.set_xlabel("Pontos previstos (m√©dia conjunto de teste)")
    st.pyplot(fig_rank, use_container_width=True)
else:
    st.info("Sem dados suficientes para montar a tabela de jogadores.")

# 11) Compara√ß√£o Time x Advers√°rio (evolu√ß√£o temporal)
st.subheader("Compara√ß√£o Time x Advers√°rio (Hist√≥rico)")
# Usar pontos do GSW vs pontos do oponente aproximado: PTS_op = PTS - PLUS_MINUS (se dispon√≠vel). Para time, n√£o temos PLUS_MINUS; usar proxy: oponente m√©dio.
series_gsw = team_log["PTS"].reset_index(drop=True)
# Gerar uma s√©rie de oponente m√©dia com ru√≠do
rng = np.random.default_rng(123)
series_opp = pd.Series((series_gsw.mean() - 3) + rng.normal(0, 7, len(series_gsw))).clip(80, 140)

fig_vs, ax = plt.subplots(figsize=(10, 4))
ax.plot(series_gsw, label="GSW PTS", color="#1976d2")
ax.plot(series_opp, label="Oponente PTS (aprox)", color="#ef6c00")
ax.set_title("Evolu√ß√£o de Pontos - GSW vs Oponente (aprox)")
ax.set_xlabel("Jogo")
ax.set_ylabel("Pontos")
ax.legend()
st.pyplot(fig_vs, use_container_width=True)

# 12) Relat√≥rio interno
st.markdown("---")
with st.expander("Relat√≥rio interno (decis√µes e an√°lise)"):
    st.markdown(
        """
        - Features: m√©dias hist√≥ricas (PTS/REB/AST/FG etc.), √∫ltimos 5 jogos e vari√°veis do jogo anterior, conforme Atividade 1. S√£o bons preditores de forma recente e tend√™ncia.
        - Arquitetura MLP: 2 camadas (64, 32) com ReLU, sa√≠da sigmoid. EarlyStopping + validation split previnem overfitting. Dropout e BatchNorm opcionais.
        - √âpocas: 300 como teto; EarlyStopping interrompe quando a valida√ß√£o estabiliza (paci√™ncia configur√°vel).
        - Otimizadores: treinamos e comparamos SGD, Adam e RMSprop. Em dados tabulares, Adam e RMSprop tendem a convergir mais r√°pido; SGD com momentum serve como baseline est√°vel.
        - M√©tricas: MAE, RMSE e R¬≤ calculadas sobre probabilidades previstas vs r√≥tulo 0/1 (interpreta√ß√£o: qu√£o bem calibrada a probabilidade est√°). Tamb√©m mostramos Accuracy e matrizes de erro.
        - Jogadores: tabela Previs√£o x Realidade para PTS/AST/REB via Regress√£o Linear por jogador, usando as mesmas features. Produzimos tamb√©m um ranking por pontos previstos.
        - Gr√°ficos: evolu√ß√£o do erro (loss/acc), matriz de erros (histograma, scatter prev/real, erro x amostra), compara√ß√£o temporal GSW vs advers√°rio aproximado.
        - Compara√ß√£o com Atividade 1: mostramos lado a lado Regress√£o Log√≠stica, Linear (0/1) e melhor MLP, com m√©tricas.
        - Observa√ß√£o: quando a API n√£o retorna dados suficientes, geramos dados sint√©ticos coerentes para manter o fluxo reprodut√≠vel.
        """
    )

st.markdown("---")
st.caption("Atividade RNA ‚Äì MLP x Regress√µes ‚Ä¢ Golden State Warriors")
